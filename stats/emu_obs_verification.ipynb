{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7d5621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## general\n",
    "import numpy as np\n",
    "import datetime\n",
    "import joblib\n",
    "import copy\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import joblib\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "\n",
    "## statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "\n",
    "## plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy.ma as ma\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import mplotutils as mpu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8792a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_obs(obs, Tref_start='1951-01-01', Tref_end='1980-01-01', Tblendglob_idx=False):\n",
    "    \"\"\" Load the observations.\n",
    "\n",
    "    Keyword argument:\n",
    "    - obs: currently implemented for \"best\" and \"cowtan\"\n",
    "    - Tref_start: starting point for the reference period with default 1951 (ie BEST Tref)\n",
    "    - Tref_end: last year still INCLUDED for the reference period with default 1980 (ie BEST Tref) (ATTENTION: different from load_data_single_mod where is the first NOT included)\n",
    "    - Tblend_idx: whether to compute the blended Tanglob anomaly or not (default = False)\n",
    "\n",
    "    Output:\n",
    "    - y: the land grid points of the anomalies of the variable on grid centered over 0 longitude (like the srexgrid) \n",
    "    - time: the time slots\n",
    "    - Tblendglob = area weighted global mean temperature (blend from SST over ocean and tas over land + sea ice)\n",
    "        \n",
    "            \n",
    "    \"\"\"\n",
    "    dir_data = '/home/tristan/mesmer/data/'\n",
    "    \n",
    "    # read in the land-sea mask\n",
    "    file_ls = 'interim_invariant_lsmask_regrid.nc' # ERA-interim mask regridded by Richard from 73x144 to 72x144\n",
    "    frac_l = xr.open_mfdataset(dir_data+file_ls) #land-sea mask of ERA-interim bilinearily interpolated \n",
    "    frac_l = frac_l.where(frac_l.lat>-60,0) # remove Antarctica from frac_l field (ie set frac l to 0)\n",
    "    idx_l=np.squeeze(frac_l.lsm.values)>0.0 # idex land #-> everything >0 I consider land\n",
    "\n",
    "    lons, lats = np.meshgrid(frac_l.lon.values,frac_l.lat.values) # the lon, lat grid (just to derive weights)   \n",
    "    wgt = norm_cos_wgt(lats) # area weights of each grid point\n",
    "    \n",
    "    \n",
    "    if obs == 'best':\n",
    "        obs_file='obs_data_25.nc' \n",
    "        ds_obs=xr.open_mfdataset(dir_data+obs_file).rename({'temperature':'tas'}).sel(time=slice('1880-01-01', '2022-01-01'))\n",
    "        time = np.arange(np.datetime64('1880-01-01'), np.datetime64('2022-01-01'), dtype='datetime64[M]')\n",
    "        ds_obs['time'] = time\n",
    "    \n",
    "    if obs == 'giss':\n",
    "        file='GISS_tas_g25.nc' \n",
    "        ds_obs=xr.open_mfdataset(dir_data+file).rename({'tempanomaly':'tas'}).sel(time=slice('1880-01-01', '2022-01-01')).drop(['time_bnds'])\n",
    "        time = np.arange(np.datetime64('1880-01-01'), np.datetime64('2022-01-01'), dtype='datetime64[M]')\n",
    "        ds_obs['time'] = time\n",
    "        ds_obs['lon'] = ds_obs['lon']-180\n",
    "        \n",
    "    T_ref = ds_obs.tas.sel(time=slice(Tref_start, Tref_end)).mean(dim='time')\n",
    "    if Tblendglob_idx == True:\n",
    "        tas=ds_obs.tas.values-T_ref.values\n",
    "        Tblendglob=np.zeros(tas.shape[0])\n",
    "        for t in np.arange(tas.shape[0]):\n",
    "            idx_valid = ~np.isnan(tas[t])\n",
    "            Tblendglob[t] = np.average(tas[t,idx_valid],weights=wgt[idx_valid]) #area weighted of available obs -> less data available at beginning        \n",
    "    \n",
    "    y=(ds_obs.tas.values-T_ref.values)[:,idx_l]\n",
    "    time=ds_obs.time.values\n",
    "    \n",
    "    if Tblendglob_idx==True:\n",
    "        return y,time,Tblendglob\n",
    "    else:\n",
    "        return y,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa90658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wgt_reg_obs(reg):\n",
    "    \n",
    "    dir_data = \"/home/tristan/mesmer/data/\"\n",
    "    file_ls = \"interim_invariant_lsmask_regrid.nc\"\n",
    "    file_obs = \"obs_data_25.nc\"\n",
    "    file_srex = \"srex-region-masks_20120709.srex_mask_SREX_masks_all.25deg.time-invariant.nc\"\n",
    "    file_srex_shape = \"referenceRegions.shp\"\n",
    "\n",
    "    df_obs=xr.open_mfdataset(dir_data+file_obs)\n",
    "\n",
    "    # SREX names ordered according to SREX mask\n",
    "    srex_names = ['ALA','CGI','WNA','CNA','ENA','CAM','AMZ','NEB','WSA','SSA','NEU','CEU','MED','SAH','WAF','EAF','SAF',\n",
    "             'NAS','WAS','CAS','TIB','EAS','SAS','SEA','NAU','SAU'] \n",
    "        \n",
    "    # srex_raw nrs from 1-26\n",
    "    srex_raw = xr.open_mfdataset(dir_data+file_srex, combine='by_coords',decode_times=False) \n",
    "    lons, lats = np.meshgrid(srex_raw.lon.values,srex_raw.lat.values) #derive the lat, lon grid\n",
    "    \n",
    "    #apply land mask\n",
    "    frac_l = xr.open_mfdataset(dir_data+file_ls, combine='by_coords',decode_times=False) #land-sea mask\n",
    "    frac_l_raw = np.squeeze(copy.deepcopy(frac_l.lsm.values))\n",
    "    frac_l = frac_l.where(frac_l.lat>-60,0)\n",
    "    idx_l=np.squeeze(frac_l.lsm.values)>0.0 \n",
    "    \n",
    "    wgt = np.cos(np.deg2rad(lats)) # area weights of each grid point\n",
    "    wgt_l = (wgt*frac_l_raw)[idx_l] # area weights for land grid points (including taking fraction land into consideration)\n",
    "    lon_pc, lat_pc = mpu.infer_interval_breaks(frac_l.lon, frac_l.lat) # the lon / lat for the plotting with pcolormesh\n",
    "    srex = (np.squeeze(srex_raw.srex_mask.values)-1)[idx_l] # srex indices on land\n",
    "\n",
    "    \n",
    "    if reg == 'Global_land':\n",
    "        idx_reg_l=np.ones(3043,dtype=bool)\n",
    "        idx_reg_l_grid = copy.deepcopy(idx_l)\n",
    "        \n",
    "    else:\n",
    "        idx_reg = srex_names.index(reg) # index region\n",
    "        idx_reg_l = (srex==idx_reg) #index land (l) gp inside specific srex region (reg)\n",
    "        srex_grid=np.zeros(idx_l.shape)\n",
    "        srex_grid[idx_l]=srex # ATTENTION: not 100% convinced yet if this is correct (I get some missing values that\n",
    "        idx_reg_l_grid = (srex_grid==idx_reg)#& idx_l \n",
    "    \n",
    "    datasets = [\"best\", \"giss\"]\n",
    "    \n",
    "    obs_tas_mean = {}\n",
    "    x = {}\n",
    "    list_x = {}\n",
    "    y_mon = {}\n",
    "    y_ann = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        obs_tas_mean[dataset] = {}\n",
    "        if dataset == \"giss\":\n",
    "            for i in np.arange(len(time_obs)):\n",
    "                idx_valid=~np.isnan(tas_GISS[i][idx_reg_l])\n",
    "                obs_tas_mean[dataset][i] = np.average(tas_GISS[i][idx_reg_l][idx_valid],weights=wgt_l[idx_reg_l][idx_valid]) \n",
    "                \n",
    "        if dataset == \"best\":\n",
    "            for i in np.arange(len(time_obs)):\n",
    "                idx_valid=~np.isnan(tas_BEST[i][idx_reg_l])\n",
    "                obs_tas_mean[dataset][i] = np.average(tas_BEST[i][idx_reg_l][idx_valid],weights=wgt_l[idx_reg_l][idx_valid])\n",
    "                \n",
    "        x[dataset] = obs_tas_mean[dataset].items()\n",
    "        list_x[dataset] = list(x[dataset])\n",
    "        y_mon[dataset] = np.array(list_x[dataset])[:, -1]\n",
    "        y_ann[dataset] = pd.DataFrame({\"time\":time_obs, \"tas\":y_mon[dataset]}).set_index(\"time\")\n",
    "        y_ann[dataset] = y_ann[dataset].resample(\"Y\").mean()\n",
    "        \n",
    "    time = np.arange(np.datetime64('1880-01-01'), np.datetime64('2022-01-01'), dtype='datetime64[Y]')\n",
    "    print('weighting complete')\n",
    "    return y_mon, y_ann, time, idx_reg_l_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3cbf8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Tan_wgt_reg_emu(reg):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the weighted averaged monthly and annual temperatures for global land, \n",
    "    or for a given SREX region.\n",
    "    Inputs:\n",
    "    - reg: region: can be 'Global_land', or any of the SREX regions\n",
    "    - emulator_innovations: output file from the emulations (with lat lon gridded data)\n",
    "    Outputs:\n",
    "    - Tan_wgt_reg_y_mon = monthly weighted mean temperature anomaly for the given region\n",
    "    - Tan_wgt_reg_y = annual weighted mean temperature anomaly for the given region\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_data = \"/home/tristan/mesmer/data/\"\n",
    "    file_ls = \"interim_invariant_lsmask_regrid.nc\"\n",
    "    file_srex = \"srex-region-masks_20120709.srex_mask_SREX_masks_all.25deg.time-invariant.nc\"\n",
    "    file_srex_shape = \"referenceRegions.shp\"\n",
    "\n",
    "    # SREX names ordered according to SREX mask\n",
    "    srex_names = ['ALA','CGI','WNA','CNA','ENA','CAM','AMZ','NEB','WSA','SSA','NEU','CEU','MED','SAH','WAF','EAF','SAF',\n",
    "             'NAS','WAS','CAS','TIB','EAS','SAS','SEA','NAU','SAU'] \n",
    "\n",
    "    # srex_raw nrs from 1-26\n",
    "    srex_raw = xr.open_mfdataset(dir_data+file_srex, combine='by_coords',decode_times=False) \n",
    "    lons, lats = np.meshgrid(srex_raw.lon.values,srex_raw.lat.values) #derive the lat, lon grid\n",
    "\n",
    "    #apply land mask\n",
    "    frac_l = xr.open_mfdataset(dir_data+file_ls, combine='by_coords',decode_times=False) #land-sea mask\n",
    "    frac_l_raw = np.squeeze(copy.deepcopy(frac_l.lsm.values))\n",
    "    frac_l = frac_l.where(frac_l.lat>-60,0)\n",
    "    idx_l=np.squeeze(frac_l.lsm.values)>0.0 \n",
    "\n",
    "    wgt = np.cos(np.deg2rad(lats)) # area weights of each grid point\n",
    "    wgt_l = (wgt*frac_l_raw)[idx_l] # area weights for land grid points (including taking fraction land into consideration)\n",
    "    lon_pc, lat_pc = mpu.infer_interval_breaks(frac_l.lon, frac_l.lat) # the lon / lat for the plotting with pcolormesh\n",
    "    srex = (np.squeeze(srex_raw.srex_mask.values)-1)[idx_l] # srex indices on land\n",
    "    \n",
    "    if reg == 'Global_land':\n",
    "        idx_reg_l=np.ones(3043,dtype=bool)\n",
    "        idx_reg_l_grid = copy.deepcopy(idx_l)\n",
    "        \n",
    "    else:\n",
    "        idx_reg = srex_names.index(reg) # index region\n",
    "        idx_reg_l = (srex==idx_reg) #index land (l) gp inside specific srex region (reg)\n",
    "        srex_grid=np.zeros(idx_l.shape)\n",
    "        srex_grid[idx_l]=srex \n",
    "        idx_reg_l_grid = (srex_grid==idx_reg)  \n",
    "\n",
    "    ## set the timespan of the emulations\n",
    "    time_emu_mon = pd.date_range(start='1910-01-01', end='2021-12-31', freq='MS')\n",
    "    time_emu = pd.date_range(start='1910-01-01', end='2021-12-31', freq='A')\n",
    "    \n",
    "    Tan_wgt_reg_y_mon = {}\n",
    "    Tan_wgt_reg_y = {}\n",
    "\n",
    "    ### calculate annual average temperature anomalies for the region\n",
    "    for run in np.arange(len(y_all)):\n",
    "        Tan_wgt_reg_y[run] = np.zeros(len(time_emu))\n",
    "        for i in np.arange(len(time_emu)):\n",
    "            Tan_wgt_reg_y[run][i] = np.average(y_all[run][i][idx_reg_l], weights=wgt_l[idx_reg_l])\n",
    "        \n",
    "    return Tan_wgt_reg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37537840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_cos_wgt(lats):\n",
    "    \n",
    "    from numpy import cos, deg2rad\n",
    "    \n",
    "    return np.cos(np.deg2rad(lats))\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c362cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, start=0, count=2):\n",
    "    \"\"\"Reshapes numpy array a by combining count dimensions, \n",
    "    starting at the dimension index start\"\"\"\n",
    "    \n",
    "    s = a.shape\n",
    "    return np.reshape(a, s[:start] + (-1,) + s[start+count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36e4e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the land mask as frac_l\n",
    "dir_in_geo_dist = '/home/tristan/mesmer/data/'\n",
    "frac_l = xr.open_mfdataset(dir_in_geo_dist + 'interim_invariant_lsmask_regrid.nc', combine='by_coords',decode_times=False)\n",
    "\n",
    "frac_l_raw = np.squeeze(copy.deepcopy(frac_l.lsm.values))  #land-sea mask of ERA-interim bilinearily interpolated \n",
    "\n",
    "frac_l = frac_l.where(frac_l.lat>-60,0)  # remove Antarctica from frac_l field (ie set frac l to 0)\n",
    "\n",
    "idx_l=np.squeeze(frac_l.lsm.values)>0.0 # idx_l = index land -> idex land #-> everything >0 we consider as land\n",
    "\n",
    "lon_pc, lat_pc = mpu.infer_interval_breaks(frac_l.lon, frac_l.lat)  ## is this needed??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b786bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighting complete\n"
     ]
    }
   ],
   "source": [
    "#### load monthly tas for each grid point ########################################\n",
    "datasets = [\"best\", \"giss\"]\n",
    "reg = 'Global_land'\n",
    "\n",
    "tas_BEST, time_obs = load_data_obs(\"best\")\n",
    "tas_GISS, time_obs = load_data_obs(\"giss\")\n",
    "\n",
    "\n",
    "### compute weighted mean TAS across all land surface gridpoints ####################\n",
    "\n",
    "obs_mon, obs_ann, time_obs_ann, idx_l_reg_gl = compute_wgt_reg_obs(reg)\n",
    "\n",
    "\n",
    "## select the correct timespan to match with the emus ###############################\n",
    "\n",
    "for dataset in datasets:\n",
    "    obs_ann[dataset] = obs_ann[dataset]['1910-12-31' : '2021-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0468d992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_ann['best'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17700e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_obs_best = {}\n",
    "\n",
    "perc_obs_best[2.5], perc_obs_best[5.0], perc_obs_best[50], perc_obs_best[95], perc_obs_best[97.5] = np.percentile(obs_ann['best'],[2.5,5.0, 50,95,97.5],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad0db4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40737414] [1.04667815]\n"
     ]
    }
   ],
   "source": [
    "print(perc_obs_best[2.5], perc_obs_best[97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27cd34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load emulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30cc7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_big = '/home/tristan/mesmer_data/output/'\n",
    "\n",
    "y_all = joblib.load(dir_data_big+'y_all_emu_fix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13d23b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 300/300 [03:05<00:00,  1.61it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_emu_gl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m tqdm(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(y_all))):\n\u001b[1;32m      4\u001b[0m     y_all_gl \u001b[38;5;241m=\u001b[39m compute_Tan_wgt_reg_emu(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobal_land\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m----> 6\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[43my_emu_gl\u001b[49m\u001b[38;5;241m.\u001b[39m dir_data_big\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_all_emu_gl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_emu_gl' is not defined"
     ]
    }
   ],
   "source": [
    "## perform regional weighting\n",
    "\n",
    "for run in tqdm(np.arange(len(y_all))):\n",
    "    y_all_gl = compute_Tan_wgt_reg_emu('Global_land')   \n",
    "    \n",
    "joblib.dump(y_all_gl. dir_data_big+'y_all_emu_gl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65077da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_all_gl[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a54fec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the percentage of each emulator run that fits in the observation quantiles:\n",
    "\n",
    "fit = np.zeros(300)\n",
    "\n",
    "for run in np.arange(len(y_all_gl)):\n",
    "    if (((-0.41 < y_all_gl[run]) & (y_all_gl[run] < 1.05)).sum())/len(y_all_gl[run])*100 >= 92:\n",
    "        fit[run] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "484a2b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d5d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
