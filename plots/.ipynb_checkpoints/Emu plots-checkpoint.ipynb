{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3faf15",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5fe542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/home/tristan/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "## general\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.externals import joblib\n",
    "import copy\n",
    "import cf_units\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datetime as dt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "## statistics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal # to compute likelihood\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from scipy.stats import shapiro  #check normalicy of seasonal trend distribution\n",
    "from scipy.optimize import curve_fit, fmin, fminbound, minimize, rosen_der, least_squares\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "##import functions for fitting\n",
    "from symfit import parameters, variables, Fit\n",
    "from symfit import pi,sqrt,log,exp,sinh\n",
    "from symfit import sin, cos\n",
    "\n",
    "\n",
    "# statistics which aren't all that nice in python\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "## my stuff\n",
    "sys.path.insert(1,'/home/tristan/mesmer/tools')\n",
    "#from tools.loading import load_data_single_mod\n",
    "from tools.processing import AR1_predict, compute_llh_cv,gaspari_cohn\n",
    "from tools.plotting import TaylorDiagram\n",
    "\n",
    "\n",
    "## plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy.ma as ma\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import mplotutils as mpu\n",
    "\n",
    "##for parallelisation\n",
    "from sklearn.externals.joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd60a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeec818",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c045ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##define function to do fitting on\n",
    "def season_mimic(x, mon, n=2):\n",
    "    \"\"\"\n",
    "    Returns a symbolic fourier series of order `n`.\n",
    "\n",
    "    :param n: Order of the fourier series.\n",
    "    :param x: Independent variable\n",
    "    :param f: Frequency of the fourier series\n",
    "    \"\"\"\n",
    "    # Make the parameter objects for all the terms\n",
    "    a = parameters(','.join(['a{}'.format(i) for i in range(1, n + 1)]))\n",
    "    b = parameters(','.join(['b{}'.format(i) for i in range(1, n + 1)]))\n",
    "    c = parameters(','.join(['c{}'.format(i) for i in range(1, n + 1)]))\n",
    "    d = parameters(','.join(['d{}'.format(i) for i in range(1, n + 1)]))\n",
    "   \n",
    "    # Construct the series\n",
    "    series =    sum((ai*x + bi)*sin(pi*i*(mon%12+1)/6)+(ci*x+di)*cos(pi*i*(mon%12+1)/6)for i,(ai, bi, ci, di) in enumerate(zip(a, b,c,d)))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f590817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_bic(x_train,mon_train,y_all_mon,max_period=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits grid point values to lowest BIC score\n",
    "    \"\"\"\n",
    "    \n",
    "    #create mask to mask out NaN values\n",
    "    mask_nan=~np.isnan(x_train)    \n",
    "    y_all_mon=y_all_mon.reshape(tot_months,-1)[mask_nan,:]\n",
    "    x_train=x_train[mask_nan]\n",
    "    mon_train=mon_train[mask_nan]\n",
    "    \n",
    "    n=len(x_train)\n",
    "    bic=np.zeros(max_period)\n",
    "    for i in range (1,max_period+1):\n",
    "        \n",
    "        num_params=2+(4*i)\n",
    "        x, mon, z = variables('x, mon, z')\n",
    "        model_dict = {z: season_mimic(x, mon, n=i)}\n",
    "        \n",
    "        if i==1:\n",
    "            fit=Fit(model_dict, x=x_train, z=y_all_mon)\n",
    "            mse=mean_squared_error(y_all_mon,fit.model(x=x_train, **fit.execute().params).z)\n",
    "        else:\n",
    "            fit=Fit(model_dict, x=x_train, mon=mon_train,z=y_all_mon)\n",
    "            mse=mean_squared_error(y_all_mon,fit.model(x=x_train, mon=mon_train,**fit.execute().params).z)\n",
    "        \n",
    "        bic[i-1] = n * log(mse) + num_params * log(n)\n",
    "        \n",
    "    order_chosen=np.where(bic==min(bic))[0][0]+1\n",
    "    \n",
    "    \n",
    "    model_dict_chosen = {z: season_mimic(x, mon, n=order_chosen)}\n",
    "    fit=Fit(model_dict_chosen, x=x_train, mon=mon_train, z=y_all_mon)\n",
    "    \n",
    "    return fit.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eaa41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_fmin(self, X, y):\n",
    "    \"\"\"Estimate the optimal parameter lambda for each feature.\n",
    "    The optimal lambda parameter for minimizing skewness is estimated on\n",
    "    each feature independently using maximum likelihood.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data used to estimate the optimal transformation parameters.\n",
    "    y : temp values to calculate lmbda as lmbda = a*y + b\n",
    "    Returns\n",
    "    -------\n",
    "    self : object\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.copy()  # force copy so that fit does not change X inplace\n",
    "\n",
    "\n",
    "    self.coeffs_ =[]\n",
    "    for i_grid in tqdm(np.arange(idx_l.sum())):\n",
    "#         print(X.shape, y.shape)\n",
    "        self.coeffs_.append(yeo_johnson_optimize_fmin(self,X[:,i_grid],y[:,i_grid]))\n",
    "        \n",
    "        \n",
    "    self.coeffs_=np.array(self.coeffs_)\n",
    "    #print(self.coeffs_.shape)\n",
    "    self.mins_ = np.amin(X, axis=0)\n",
    "    self.maxs_ = np.amax(X, axis=0)\n",
    "    #print(self.coeffs_)\n",
    "    \n",
    "    if self.standardize:\n",
    "        self._scaler = StandardScaler(copy=True)\n",
    "        self._scaler.fit(X)\n",
    "        \n",
    "    return self\n",
    "    \n",
    "def transform_fmin(self, X, y):\n",
    "        \"\"\"Apply the power transform to each feature using the fitted lambdas.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data to be transformed using a power transformation.\n",
    "        Returns\n",
    "        -------\n",
    "        X_trans : array-like, shape (n_samples, n_features)\n",
    "            The transformed data.\n",
    "        \"\"\"\n",
    "\n",
    "        lambdas=get_yeo_johnson_lambdas(self.coeffs_,y)\n",
    "        \n",
    "        X_trans=np.zeros_like(X)\n",
    "        for i, lmbda in enumerate(lambdas.T):\n",
    "            for j,j_lmbda in enumerate(lmbda):\n",
    "                with np.errstate(invalid='ignore'):  # hide NaN warnings\n",
    "                    X_trans[j, i] = self._yeo_johnson_transform(X[j, i], j_lmbda)\n",
    "\n",
    "        if self.standardize:\n",
    "            X_trans = self._scaler.transform(X_trans)\n",
    "\n",
    "        return X_trans\n",
    "\n",
    "def inverse_transform_fmin(self, X, y):\n",
    "        \"\"\"Apply the inverse power transformation using the fitted lambdas.\n",
    "        The inverse of the Yeo-Johnson transformation is given by::\n",
    "            if X >= 0 and lambda_ == 0:\n",
    "                X = exp(X_trans) - 1\n",
    "            elif X >= 0 and lambda_ != 0:\n",
    "                X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "            elif X < 0 and lambda_ != 2:\n",
    "                X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
    "            elif X < 0 and lambda_ == 2:\n",
    "                X = 1 - exp(-X_trans)\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The transformed data.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The original data\n",
    "        \"\"\"\n",
    "\n",
    "        if self.standardize:\n",
    "            X = self._scaler.inverse_transform(X) \n",
    "        \n",
    "        X_inv = np.zeros_like(X)\n",
    "        \n",
    "        lambdas=get_yeo_johnson_lambdas(self.coeffs_,y)\n",
    "        for i, lmbda in enumerate(lambdas.T):\n",
    "            for j,j_lmbda in enumerate(lmbda):\n",
    "                with np.errstate(invalid='ignore'):  # hide NaN warnings\n",
    "                    X_inv[j, i] = self._yeo_johnson_inverse_transform(X[j, i], j_lmbda)\n",
    "            X_inv[:,i]=np.where(X_inv[:,i]<self.mins_[i],self.mins_[i],X_inv[:,i])\n",
    "            X_inv[:,i]=np.where(X_inv[:,i]>self.maxs_[i],self.maxs_[i],X_inv[:,i])\n",
    "\n",
    "        return X_inv\n",
    "\n",
    "def yeo_johnson_optimize_fmin(self, x, y):\n",
    "    \"\"\"Find and return optimal lambda parameter of the Yeo-Johnson\n",
    "    transform by MLE, for observed data x.\n",
    "    Like for Box-Cox, MLE is done via the brent optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def _neg_log_likelihood(coeff):\n",
    "        \"\"\"Return the negative log likelihood of the observed data x as a\n",
    "        function of lambda.\"\"\"\n",
    "        lambdas=2/(1+coeff[0]*np.exp(y*coeff[1]))\n",
    "        \n",
    "        x_trans =np.zeros_like(x)\n",
    "#         print(x.shape)\n",
    "        #print(lambdas.shape)\n",
    "        for i, lmbda in enumerate(lambdas):\n",
    "            x_trans[i] = self._yeo_johnson_transform(x[i], lmbda)\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "        loglike = -n_samples / 2 * np.log(x_trans.var())\n",
    "        loglike += ((lambdas - 1) * np.sign(x) * np.log1p(np.abs(x))).sum()\n",
    "\n",
    "        return -loglike\n",
    "\n",
    "    # the computation of lambda is influenced by NaNs so we need to\n",
    "    # get rid of them\n",
    "    x = x[~np.isnan(x)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    # choosing bracket -2, 2 like for boxcox\n",
    "    bounds=np.c_[[0,0], [1,0.1]]\n",
    "    \n",
    "    return minimize(_neg_log_likelihood, np.array([0.01,0.001]), bounds=bounds, method='SLSQP',jac=rosen_der \n",
    "               ).x\n",
    "\n",
    "def get_yeo_johnson_lambdas(coeffs,y):\n",
    "\n",
    "    lambdas=np.zeros_like(y)\n",
    "    i=0\n",
    "    for a,b in zip(coeffs,y.T):\n",
    "        \n",
    "        lambdas[:,i]=2/(1+a[0]*np.exp(b*a[1]))\n",
    "        i+=1\n",
    "        \n",
    "    lambdas=np.where(lambdas<0,0,lambdas)\n",
    "    lambdas=np.where(lambdas>2,2,lambdas)\n",
    "    \n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b245bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def power_fit(residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        power_trans = fit_fmin(PowerTransformer(method='yeo-johnson'),residue.reshape(-1, idx_l.sum()),y)\n",
    "    else:\n",
    "        power_trans = PowerTransformer(method='yeo-johnson').fit(residue.reshape(-1, idx_l.sum()))\n",
    "    \n",
    "    return power_trans\n",
    "    \n",
    "def power_transform(mod, residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        residue_trans = transform_fmin(mod,residue.reshape(-1, idx_l.sum()),y).reshape(-1,idx_l.sum())\n",
    "    else:\n",
    "        residue_trans = mod.transform(residue.reshape(-1, idx_l.sum())).reshape(-1,idx_l.sum())\n",
    "            \n",
    "    return residue_trans  \n",
    "\n",
    "def power_inv_transform(mod, residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        residue_inv_trans = inverse_transform_fmin(mod,residue.reshape(-1, idx_l.sum()),y).reshape(-1,idx_l.sum())\n",
    "    else:\n",
    "        residue_inv_trans = mod.inverse_transform(residue.reshape(-1, idx_l.sum())).reshape(-1,idx_l.sum())\n",
    "            \n",
    "    return residue_inv_trans  \n",
    "\n",
    "def compute_llh_cv(res_tr,res_cv,phi):\n",
    "    \"\"\" Compute sum of log likelihood of a set of residuals based on a covariance matrix derived from a different set (of timeslots) of residuals\n",
    "    \n",
    "    Keyword arguments:\n",
    "        - res_tr: the residual of the training run lacking a specific fold after removing the local mean response (nr ts x nr gp). Nans must be removed before\n",
    "        - res_cv: the residual of a fold which was removed from the training run\n",
    "        - phi: matrix to localize the covariance matrix based on a specific localisation radius and distance information (phi = output of fct gaspari_cohen(geo_dist/L))\n",
    "    \n",
    "    Output:\n",
    "        - llh_innov_cv: sum of the log likelihood over the cross validation time slots\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ecov_res_tr = np.cov(res_tr,rowvar=False)\n",
    "    cov_res_tr=phi*ecov_res_tr\n",
    "    \n",
    "    mean_0 = np.zeros(phi.shape[0]) \n",
    "\n",
    "    llh_innov_cv=np.sum(multivariate_normal.logpdf(res_cv,mean=mean_0, cov=cov_res_tr,allow_singular=True))\n",
    "\n",
    "    return llh_innov_cv   \n",
    "\n",
    "def leave_one_out(L_set,nr_folds,residue_trans,idx_fo_tot,phi):\n",
    "    \n",
    "    def folds_calc(idx_fo,residue_trans,phi,L):\n",
    "    \n",
    "        res_tot_est = residue_trans[~idx_fo] \n",
    "        res_tot_fo=residue_trans[idx_fo]\n",
    "\n",
    "        llh_cv=compute_llh_cv(res_tot_est,res_tot_fo,phi[L])\n",
    "        \n",
    "        return llh_cv\n",
    "    \n",
    "    idx_L=0\n",
    "    L = L_set[idx_L]\n",
    "    \n",
    "    df_llh_cv={}\n",
    "    df_llh_cv['llh_max']=-10000\n",
    "    df_llh_cv['all']={}\n",
    "    df_llh_cv['sum']={}\n",
    "    df_llh_cv['L_sel']=L_set[idx_L]\n",
    "    \n",
    "    while (L-df_llh_cv['L_sel']<=250) and (df_llh_cv['L_sel']<L_set[-1]):\n",
    "        L = L_set[idx_L]\n",
    "        print('start with L ',L)\n",
    "        df_llh_cv['all'][L]={}\n",
    "        df_llh_cv['sum'][L]=0\n",
    "        for i_fold_par in tqdm(np.arange(len(idx_fo_tot.keys()))):\n",
    "            df_llh_cv['all'][L][i_fold_par]=folds_calc(idx_fo_tot[i_fold_par],residue_trans,phi,L)\n",
    "            df_llh_cv['sum'][L] += df_llh_cv['all'][L][i_fold_par]\n",
    "            \n",
    "        if df_llh_cv['sum'][L]>df_llh_cv['llh_max']:\n",
    "            df_llh_cv['L_sel']=L\n",
    "            df_llh_cv['llh_max']=df_llh_cv['sum'][L]\n",
    "            print('currently selected L=',df_llh_cv['L_sel'])\n",
    "\n",
    "        idx_L+=1  \n",
    "    return df_llh_cv\n",
    "\n",
    "def lin_func(x, a, b):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da50de",
   "metadata": {},
   "source": [
    "# Load data and train harmonic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170a95a",
   "metadata": {},
   "source": [
    "## Load land mask and create land mask index (idx_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a23889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the land mask as frac_l\n",
    "dir_in_geo_dist = '/home/tristan/mesmer/data/'\n",
    "frac_l = xr.open_mfdataset(dir_in_geo_dist + 'interim_invariant_lsmask_regrid.nc', combine='by_coords',decode_times=False)\n",
    "\n",
    "frac_l_raw = np.squeeze(copy.deepcopy(frac_l.lsm.values))  #land-sea mask of ERA-interim bilinearily interpolated \n",
    "\n",
    "frac_l = frac_l.where(frac_l.lat>-60,0)  # remove Antarctica from frac_l field (ie set frac l to 0)\n",
    "\n",
    "idx_l=np.squeeze(frac_l.lsm.values)>0.0 # idx_l = index land -> idex land #-> everything >0 we consider as land\n",
    "\n",
    "lon_pc, lat_pc = mpu.infer_interval_breaks(frac_l.lon, frac_l.lat)  ## is this needed??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd1ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating seasonal trends\n",
      "(1344, 3043)\n",
      "(1344, 3043)\n",
      "Output folder selected, training starting now, please be patient... \n",
      " Go outside, take a long walk, have some food, take a nap, enjoy your life for a few hours...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869b7969b55d4c1e93ed0e06ba37efdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3043 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5284/4173576813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m seasonal_model_exec= Parallel(n_jobs=12)(delayed(fit_to_bic)(np.repeat(y_all[:,i],12,axis=0),\n\u001b[1;32m     58\u001b[0m                                                                    mon_train, y_all_mon[:,i])\n\u001b[0;32m---> 59\u001b[0;31m                                                for i in tqdm(np.arange(idx_l.sum())))\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseasonal_model_exec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdir_out_data_mod\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'seasonal_trend.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mesmer-env/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mesmer-env/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mesmer-env/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##Start training for monthly downscaling\n",
    "\n",
    "y_all_mon={}\n",
    "y_all={}\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "print(\"calculating seasonal trends\")\n",
    "        \n",
    "# prepare the inputs as array\n",
    "BEST_data = 'obs_data_25.nc'\n",
    "data_mask = 'interim_invariant_lsmask_regrid.nc'       \n",
    "\n",
    "df_obs = xr.open_mfdataset(dir_in_geo_dist+BEST_data).roll(lon=72) #open observation data\n",
    "        \n",
    "#create the climatology values array\n",
    "nr_years = 112\n",
    "tot_months = 12*nr_years \n",
    "\n",
    "y_ma = np.zeros((tot_months,idx_l.sum()))  #create emtpy array with correct shape\n",
    "for i in range(tot_months):\n",
    "    y_ma[i] = df_obs.climatology.values[i%12,idx_l]    #fill climatology values in the array\n",
    "\n",
    "print(y_ma.shape)\n",
    "#create test data over date range - here, 112 years so 1910 incl. to 2022. \n",
    "data_test = np.array([df_obs.temperature.values[720:2064,idx_l]])\n",
    "data_test = data_test.reshape(12*nr_years,idx_l.sum())\n",
    "        \n",
    "#load in monthly temperature values by adding the temp anomolies to the climatology\n",
    "y_all_mon = np.add(y_ma, data_test)\n",
    "\n",
    "# now subtract the yearly average climatology so we are left with the monthly anomolies (monthly value minus year average)\n",
    "y_all_mon = np.subtract(y_all_mon, np.reshape(np.tile(np.mean(df_obs.climatology.values[:,idx_l],axis=0),tot_months),(tot_months,idx_l.sum())))\n",
    "print(y_all_mon.shape)\n",
    "\n",
    "#calculate annual avergage temperature values - here we only need the shape for now\n",
    "y_all = np.mean(y_all_mon.reshape(-1,12,idx_l.sum()),axis=1) \n",
    "\n",
    "##Get directory to store outputs\n",
    "dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "        \n",
    "print('Output folder selected, training starting now, please be patient... \\n Go outside, take a long walk, have some food, take a nap, enjoy your life for a few hours...')\n",
    "\n",
    "if not os.path.exists(dir_out_data_mod):\n",
    "    os.makedirs(dir_out_data_mod)\n",
    "    print('created dir:',dir_out_data_mod)\n",
    "         \n",
    "##prepare training data\n",
    "months=np.arange(1,13)\n",
    "mon_train=np.tile(months,y_all.shape[0])\n",
    "\n",
    "#final check of shapes\n",
    "#print (np.repeat(y_all[:,0],12).shape,mon_train.shape and y_all_mon[:,0].shape)\n",
    "\n",
    "seasonal_model_exec= Parallel(n_jobs=12)(delayed(fit_to_bic)(np.repeat(y_all[:,i],12,axis=0),\n",
    "                                                                   mon_train, y_all_mon[:,i])\n",
    "                                               for i in tqdm(np.arange(idx_l.sum())))\n",
    "                    \n",
    "joblib.dump(seasonal_model_exec,dir_out_data_mod+'seasonal_trend.pkl')\n",
    "        \n",
    "print(\"finito!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ab49a1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5284/1657016565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0my_ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_months\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#create emtpy array with correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_months\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0my_ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclimatology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx_l\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#fill climatology values in the array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#create test data over date range - here, 127 years so 1910 incl. to 2022 incl.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "##Extract test and train results\n",
    "seasonal_mod={}\n",
    "order_chosen={}\n",
    "\n",
    "model=\"BEST observations\"\n",
    "models=[model]\n",
    "\n",
    "train_results_all={}\n",
    "train_residue_all={}\n",
    "y_all_mon={}\n",
    "y_all={}\n",
    "\n",
    "#create the climatology values array\n",
    "nr_years = 112\n",
    "tot_months = 12*nr_years \n",
    "\n",
    "# seasonal_mod={}\n",
    "# order_chosen={}\n",
    "\n",
    "for model in models:\n",
    "\n",
    "########## THIS SECTION CAN BE COMMNENTED OUT IF RUNNING DIRECTLY AFTER RUNNING THE PREVIOUS CELL ##############    \n",
    "#Get directory to store outputs\n",
    "    dir_in_data_mod = '/home/tristan/mesmer/data/'\n",
    "\n",
    "    # prepare the inputs as array\n",
    "    BEST_data = 'obs_data_25.nc'\n",
    "    data_mask = 'interim_invariant_lsmask_regrid.nc'       \n",
    "    \n",
    "    df_obs = xr.open_mfdataset(dir_in_geo_dist+BEST_data).roll(lon=72) #open observation data\n",
    "        \n",
    "    #create the climatology values array\n",
    "    y_ma = np.zeros((tot_months,idx_l.sum()))  #create emtpy array with correct shape\n",
    "    for i in range(tot_months):\n",
    "        y_ma[i] = df_obs.climatology.values[i%12,idx_l]    #fill climatology values in the array\n",
    "\n",
    "    #create test data over date range - here, 127 years so 1910 incl. to 2022 incl. \n",
    "    ## 1344 is the number of months from 1910 until 2022\n",
    "    data_test = np.nan_to_num(np.array([df_obs.temperature.values[720:2064,idx_l]]))\n",
    "    data_test = data_test.reshape(tot_months,idx_l.sum())\n",
    "    \n",
    "    print(data_test.shape)\n",
    "        \n",
    "    #load in monthly temperature values by adding the temp anomolies to the climatology\n",
    "    y_all_mon[model] = np.add(y_ma, data_test)     \n",
    "    \n",
    "    # now subtract the yearly average climatology so we are left with residuals\n",
    "    y_all_mon[model] = y_all_mon[model] - np.reshape(np.tile(np.mean(df_obs.climatology.values[:,idx_l],axis=0),tot_months),(tot_months,idx_l.sum()))   \n",
    "\n",
    "################## UNTIL HERE #########################             \n",
    "        \n",
    "    #calculate annual average temperature values- here we use nanmean to calculate the annual means for each gridpoint but skipping any Nan values\n",
    "    y_all[model] = np.mean(y_all_mon[model].reshape(-1,12,idx_l.sum()),axis=1)\n",
    "    print(y_all[model].shape)\n",
    "    \n",
    "############ here begins the actual training ##############################\n",
    "    \n",
    "#     print (\"Getting seasonal trends for\", model)\n",
    "    \n",
    "    nans = np.isnan(y_all_mon[model]).sum()\n",
    "    print(nans)\n",
    "    \n",
    "#     if nans == 0:\n",
    "#         print(\"No Nan values, moving on...\")\n",
    "        \n",
    "#     else:\n",
    "#         print(\"Imputing Nan values...\")\n",
    "#         imp_mean = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "#         SimpleImputer()\n",
    "#         y_all[model] = imp_mean.fit_transform(y_all[model])\n",
    "#         print(y_all[model].shape)\n",
    "        \n",
    "#         nans = np.isnan(y_all[model]).sum()\n",
    "#         print(nans)\n",
    "    dir_in_data_mod = '/home/tristan/mesmer/output/'\n",
    "    seasonal_mod[model]=joblib.load(dir_in_data_mod+'seasonal_trend.pkl')\n",
    "    \n",
    "    if os.path.exists(os.path.join(dir_in_data_mod,'seasonal_training_results.pkl')):\n",
    "        train_results_all[model]=joblib.load(dir_in_data_mod+'seasonal_training_results.pkl')\n",
    "        \n",
    "        train_residue_all[model]=np.subtract(y_all_mon[model], train_results_all[model])\n",
    "        train_residue_all[model]=train_residue_all[model].reshape(-1,12,idx_l.sum())\n",
    "\n",
    "    else:\n",
    "        train_results_all[model]=np.zeros_like(y_all_mon[model] ) \n",
    "        months=np.arange(1,13)\n",
    "        mon_train=np.tile(months,y_all[model].shape[0] )\n",
    "        \n",
    "        for i in tqdm(np.arange(idx_l.sum())):\n",
    "            x_train=np.repeat(y_all[model][:,i],12)\n",
    "            x, mon, z = variables('x, mon, z')\n",
    "            train_results_all[model][:,i]=seasonal_mod[model][i].model(x=x_train, mon=np.tile(months,nr_years),**seasonal_mod[model][i].params).z\n",
    "    \n",
    "        ##store training results\n",
    "        joblib.dump(train_results_all[model],dir_in_data_mod+'seasonal_training_results.pkl')  \n",
    "\n",
    "    ###plot order of harmonics chosen\n",
    "    order_chosen[model]=np.zeros([idx_l.sum()]) \n",
    "    \n",
    "    for i in np.arange(idx_l.sum()):\n",
    "        fit_result=seasonal_mod[model][i]\n",
    "        order_chosen[model][i]=int((len(fit_result.params)+2)/4)-1\n",
    "      \n",
    "    \n",
    "    fig=plt.figure(figsize=(10,20))\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    plt.rcParams.update({'mathtext.default':'regular'}) \n",
    "    plt.rcParams.update({'mathtext.default':'it'}) \n",
    "    \n",
    "    ax=fig.add_subplot(1,1,1,projection=ccrs.Robinson(central_longitude=0))\n",
    "\n",
    "    y_ma = np.zeros(idx_l.shape)\n",
    "    y_ma = ma.masked_array(y_ma, mask=idx_l==False)\n",
    "    y_ma[idx_l]=order_chosen[model]-1\n",
    "    \n",
    "    print(y_ma.shape)\n",
    "    \n",
    "    mesh_1=ax.pcolormesh(lon_pc, lat_pc, y_ma,  cmap=plt.cm.get_cmap('inferno_r', 7),vmin=0,vmax=7,transform=ccrs.PlateCarree(),rasterized=True)\n",
    "    ax.set_title('Fourier Series %s'%model,y=1.02,fontsize=14)\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    cbar=plt.colorbar(mesh_1,ax=[ax],orientation='horizontal',ticks=(np.arange(8)+0.5),shrink=0.8,pad=0.02,aspect=25)\n",
    "    cbar.set_label('Order')  \n",
    "    cbar.ax.set_xticklabels(np.arange(8))\n",
    "    \n",
    "    ax.coastlines()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## run a Shapiro Wilks test on the training residues to get the p-values for each grid point in each month\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "p_vals=np.zeros([12,idx_l.sum()])\n",
    "for i in range(12):        \n",
    "    p_vals[i,:] = np.hstack(([shapiro(train_residue_all[model].reshape(nr_years,12,idx_l.sum())[:,i,i_grid])[1] for i_grid in np.arange(idx_l.sum())]))\n",
    "\n",
    "p_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "months=['Jan','Feb','Mar','Apr','May','June','July','Aug','Sept','Oct','Nov','Dec']\n",
    "\n",
    "white = np.array([248/256, 230/256, 200/256, 1])\n",
    "blue = np.array([30/256, 70/256, 130/256, 1])\n",
    "# colors = np.vstack((white,blue))\n",
    "colors = ['#efefdb', '#e55c30']\n",
    "\n",
    "mymap = mpl.colors.LinearSegmentedColormap.from_list('my_colormap', colors, N=2)\n",
    "bounds = [ -1.5, 0.5, 1.5]\n",
    "norm = mpl.colors.BoundaryNorm(bounds, mymap.N)\n",
    "n_col = 3\n",
    "n_row = 4\n",
    "\n",
    "fs_title=16\n",
    "\n",
    "fig=plt.figure(figsize=(n_col*14, n_row * 18))\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams.update({'mathtext.default':'regular'})\n",
    "plt.rcParams.update({'mathtext.default':'it'})\n",
    "\n",
    "grid = plt.GridSpec(n_row*12+3, n_col*15+4, wspace=0.25, hspace=0) \n",
    "\n",
    "# create a grid for the subplots #0.12\n",
    "\n",
    "# props = dict(boxstyle='round', facecolor='yellow', alpha=0.5)\n",
    "\n",
    "for i in range(12):    \n",
    "    if i%3==0:\n",
    "        i_y=0\n",
    "        ax = plt.subplot(grid[i+1:i+3,i_y:i_y*10+10], projection=ccrs.Robinson(central_longitude=0))\n",
    "        \n",
    "    else:\n",
    "        ax = plt.subplot(grid[i-(i%3)+1:i-(i%3)+3,i_y*10:i_y*10+10], projection=ccrs.Robinson(central_longitude=0))\n",
    "            \n",
    "    i_y+=1\n",
    "    \n",
    "    y_ma = np.zeros(idx_l.shape)\n",
    "    y_ma = ma.masked_array(y_ma, mask=idx_l==False)   \n",
    "    reject = multipletests(p_vals[i,:],alpha=0.1,method='fdr_bh')[0]\n",
    "    \n",
    "    y_ma[idx_l] = reject\n",
    "    mesh=ax.pcolormesh(lon_pc, lat_pc, y_ma,  cmap=mymap,transform=ccrs.PlateCarree(), norm=norm, vmin=0, vmax=0.1, rasterized=True)\n",
    "    ax.set_title('BEST Obs. %s : %.2f' %(months[i],(reject.sum()/idx_l.sum()*100)) +'%',y=1.02,fontsize=14)\n",
    "    axcbar = plt.subplot(grid[i-(i%3)+3:i-(i%3)+4,5:25])\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    plt.axis('off')\n",
    "    \n",
    "\n",
    "cbar=plt.colorbar(mesh, orientation=\"horizontal\", fraction=0.65,aspect=75, ticks = [0, 1])\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_ticklabels(['not rejected','rejected'])\n",
    "cbar.set_label('p-value [-]',fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform variables first\n",
    "\n",
    "power_trans={}\n",
    "train_residue_trans={}\n",
    "train_residue_all={}\n",
    "test_residue_all={}\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    print('Training power transformer for,', model)\n",
    "    train_residue_all[model]=np.subtract(y_all_mon[model], train_results_all[model])\n",
    "    train_residue_all[model]=train_residue_all[model].reshape(-1,12,idx_l.sum())\n",
    "    \n",
    "    \n",
    "#     x = np.array(train_residue_all[model])\n",
    "#     print(train_residue_all[model][:,i_mon,:].shape)\n",
    "    \n",
    "    power_trans[model]=[]\n",
    "    for i_mon in tqdm(range(12)):\n",
    "        power_trans[model].append(power_fit(train_residue_all[model][:,i_mon,:],y_all[model]))\n",
    "\n",
    "    train_residue_trans[model]=Parallel(n_jobs=12,verbose=10)(delayed(power_transform)(power_trans[model][i_mon],train_residue_all[model][:,i_mon,:],y_all[model]) for i_mon in range(12))\n",
    "    \n",
    "    temp_residue_trans=np.zeros_like(train_residue_all[model])\n",
    "    for i_mon in range(12):\n",
    "        temp_residue_trans[:,i_mon,:]=train_residue_trans[model][i_mon]\n",
    "    train_residue_trans[model]=temp_residue_trans\n",
    "    \n",
    "    dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "    \n",
    "    if not os.path.exists(dir_out_data_mod):\n",
    "        os.makedirs(dir_out_data_mod)\n",
    "        print('created dir:',dir_out_data_mod)\n",
    "    joblib.dump(train_residue_trans[model],dir_out_data_mod+'train_residue_trans.pkl')\n",
    "\n",
    "    if not os.path.exists(dir_out_data_mod):\n",
    "        os.makedirs(dir_out_data_mod)\n",
    "        print('created dir:',dir_out_data_mod)\n",
    "    joblib.dump(power_trans[model],dir_out_data_mod+'yeo_johnson_pt_fmin_log.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Best observations\"\n",
    "models=[model]\n",
    "\n",
    "temp_residue_trans={}\n",
    "train_residue_trans={}\n",
    "dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "\n",
    "for model in tqdm(models):\n",
    "    train_residue_all[model]=joblib.load(dir_out_data_mod+'seasonal_training_results.pkl')\n",
    "    \n",
    "    temp_residue_trans=np.zeros_like(train_residue_all[model])\n",
    "    \n",
    "    power_trans[model] = joblib.load(dir_out_data_mod+'yeo_johnson_pt_fmin_log.pkl')\n",
    "    \n",
    "    train_residue_trans[model]=Parallel(n_jobs=12,verbose=10)(delayed(power_transform)(power_trans[model][i_mon],train_residue_all[model][:,i_mon,:],y_all[model]) for i_mon in tqdm(range(12)))\n",
    "    \n",
    "    for i_mon in range(12):\n",
    "        temp_residue_trans[:,i_mon,:]=train_residue_trans[model][i_mon]\n",
    "\n",
    "    train_residue_trans[model]=temp_residue_trans\n",
    "    \n",
    "    dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "    \n",
    "    if not os.path.exists(dir_out_data_mod):\n",
    "        os.makedirs(dir_out_data_mod)\n",
    "        print('created dir:',dir_out_data_mod)\n",
    "    \n",
    "    joblib.dump(train_residue_trans[model],dir_out_data_mod+'train_residue_trans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd5e96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
