{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f826a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## general\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.externals import joblib\n",
    "import copy\n",
    "import cf_units\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "from tqdm import notebook.tqdm as tqdm\n",
    "import datetime as dt\n",
    "import matplotlib as mpl\n",
    "\n",
    "## statistics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal # to compute likelihood\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from scipy.stats import shapiro  #check normalicy of seasonal trend distribution\n",
    "from scipy.optimize import curve_fit, fmin, fminbound, minimize, rosen_der, least_squares\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "##import functions for fitting\n",
    "from symfit import parameters, variables, Fit\n",
    "from symfit import pi,sqrt,log,exp,sinh\n",
    "from symfit import sin, cos\n",
    "\n",
    "\n",
    "# statistics which aren't all that nice in python\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "## my stuff\n",
    "sys.path.insert(1,'/home/tristan/mesmer/tools')\n",
    "#from tools.loading import load_data_single_mod\n",
    "from tools.processing import AR1_predict, compute_llh_cv,gaspari_cohn\n",
    "from tools.plotting import TaylorDiagram\n",
    "\n",
    "\n",
    "## plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "import cartopy.crs as ccrs\n",
    "import mplotutils as mpu\n",
    "\n",
    "##for parallelisation\n",
    "from sklearn.externals.joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3635c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def power_fit(residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        power_trans = fit_fmin(PowerTransformer(method='yeo-johnson'),residue.reshape(-1, idx_l.sum()),y)\n",
    "    else:\n",
    "        power_trans = PowerTransformer(method='yeo-johnson').fit(residue.reshape(-1, idx_l.sum()))\n",
    "    \n",
    "    return power_trans\n",
    "    \n",
    "def power_transform(mod, residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        residue_trans = transform_fmin(mod,residue.reshape(-1, idx_l.sum()),y).reshape(-1,idx_l.sum())\n",
    "    else:\n",
    "        residue_trans = mod.transform(residue.reshape(-1, idx_l.sum())).reshape(-1,idx_l.sum())\n",
    "            \n",
    "    return residue_trans  \n",
    "\n",
    "def power_inv_transform(mod, residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        residue_inv_trans = inverse_transform_fmin(mod,residue.reshape(-1, idx_l.sum()),y).reshape(-1,idx_l.sum())\n",
    "    else:\n",
    "        residue_inv_trans = mod.inverse_transform(residue.reshape(-1, idx_l.sum())).reshape(-1,idx_l.sum())\n",
    "            \n",
    "    return residue_inv_trans  \n",
    "\n",
    "def compute_llh_cv(res_tr,res_cv,phi):\n",
    "    \"\"\" Compute sum of log likelihood of a set of residuals based on a covariance matrix derived from a different set (of timeslots) of residuals\n",
    "    \n",
    "    Keyword arguments:\n",
    "        - res_tr: the residual of the training run lacking a specific fold after removing the local mean response (nr ts x nr gp). Nans must be removed before\n",
    "        - res_cv: the residual of a fold which was removed from the training run\n",
    "        - phi: matrix to localize the covariance matrix based on a specific localisation radius and distance information (phi = output of fct gaspari_cohen(geo_dist/L))\n",
    "    \n",
    "    Output:\n",
    "        - llh_innov_cv: sum of the log likelihood over the cross validation time slots\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ecov_res_tr = np.cov(res_tr,rowvar=False)\n",
    "    cov_res_tr=phi*ecov_res_tr\n",
    "    \n",
    "    mean_0 = np.zeros(phi.shape[0]) # we want the mean of the res to be 0\n",
    "\n",
    "    llh_innov_cv=np.sum(multivariate_normal.logpdf(res_cv,mean=mean_0, cov=cov_res_tr,allow_singular=True))\n",
    "\n",
    "    return llh_innov_cv   \n",
    "\n",
    "def leave_one_out(L_set,nr_folds,residue_trans,idx_fo_tot,phi):\n",
    "    \n",
    "    def folds_calc(idx_fo,residue_trans,phi,L):\n",
    "    \n",
    "        res_tot_est = residue_trans[~idx_fo] \n",
    "        res_tot_fo=residue_trans[idx_fo]\n",
    "\n",
    "        llh_cv=compute_llh_cv(res_tot_est,res_tot_fo,phi[L])\n",
    "        \n",
    "        return llh_cv\n",
    "    \n",
    "    idx_L=0\n",
    "    L = L_set[idx_L]\n",
    "    \n",
    "    df_llh_cv={}\n",
    "    df_llh_cv['llh_max']=-10000\n",
    "    df_llh_cv['all']={}\n",
    "    df_llh_cv['sum']={}\n",
    "    df_llh_cv['L_sel']=L_set[idx_L]\n",
    "    \n",
    "    while (L-df_llh_cv['L_sel']<=250) and (df_llh_cv['L_sel']<L_set[-1]): # based on experience I know that once stop selecting larger \n",
    "            #loc radii, will not start again -> no point in looping through everything, better to stop once max is \n",
    "            #reached (to avoid singular matrices)\n",
    "        L = L_set[idx_L]\n",
    "        print('start with L ',L)\n",
    "        df_llh_cv['all'][L]={}\n",
    "        df_llh_cv['sum'][L]=0\n",
    "        for i_fold_par in tqdm(np.arange(len(idx_fo_tot.keys()))):\n",
    "            df_llh_cv['all'][L][i_fold_par]=folds_calc(idx_fo_tot[i_fold_par],residue_trans,phi,L)\n",
    "            df_llh_cv['sum'][L] += df_llh_cv['all'][L][i_fold_par]\n",
    "            \n",
    "       \n",
    "        #df_llh_cv['all'][L]=Parallel(n_jobs=10,verbose=10)(delayed(folds_calc)(idx_fo_tot[i],residue_trans,phi,L)for i in np.arange(len(idx_fo_tot.keys())))\n",
    "        \n",
    "        #print('rest tot fo shape ',res_tot_fo.shape,'res_tot_est shape ',res_tot_est.shape)\n",
    "        if df_llh_cv['sum'][L]>df_llh_cv['llh_max']:\n",
    "            df_llh_cv['L_sel']=L\n",
    "            df_llh_cv['llh_max']=df_llh_cv['sum'][L]\n",
    "            print('currently selected L=',df_llh_cv['L_sel'])\n",
    "\n",
    "        idx_L+=1  \n",
    "    return df_llh_cv\n",
    "\n",
    "def lin_func(x, a, b):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_in_geo_dist = '/home/tristan/mesmer/data/'\n",
    "geo_dist=np.load(dir_in_geo_dist + 'geo_dist.npy')\n",
    "#L_set = [1500,1750,2000,2250,2500,2750,3000,3250,3500,3750,4000,4250,4500,4750,5000,5250,5500] \n",
    "L_set = [1500,1750,2000,2250,2500,2750,3000,3250,3500,3750,4000,4250,4500,4750,5000,5250,5500,6000,6250,6500,7000,7500,8000,8500]\n",
    "    # high loc radius stms does not work because of singular matrices. I try to stop cv now once I obtain declining\n",
    "    # likelihoods. I am not sure whether it will work without issues yet\n",
    "\n",
    "#L_set = [9000,9250] # for ['MCM-UA-1-0']\n",
    "#L_set = [10750]\n",
    "#L_set = [500,750,1000,1250,1500] # for re-doing emulations of INM & IPSL models\n",
    "phi = {}\n",
    "for L in tqdm(L_set):\n",
    "    phi[L] = np.zeros(geo_dist.shape)\n",
    "\n",
    "    for i in tqdm(np.arange(geo_dist.shape[0])):\n",
    "        for j in np.arange(geo_dist.shape[1]):\n",
    "            phi[L][i,j]=gaspari_cohn(geo_dist[i,j]/L)\n",
    "        if i % 1000 == 0:\n",
    "                print('done with L:',L,'i:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c423cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Best observations\"\n",
    "models=[model]\n",
    "\n",
    "df_llh_cv_all={}\n",
    "L_set = [1500,1750,2000,2250,2500,2750,3000,3250,3500,3750,4000,4250,4500,4750,5000,5250,5500] \n",
    "\n",
    "coeff_0={}\n",
    "coeff_1={}\n",
    "\n",
    "train_residue_all_spat={}\n",
    "train_residue_trans={}\n",
    "power_trans={}\n",
    "\n",
    "nr_years=112\n",
    "\n",
    "dir_in_data_mod = '/home/tristan/mesmer/output/'\n",
    "\n",
    "#L_set = [500,750,1000,1250,1500]\n",
    "#L_set = [9000,9250]\n",
    "\n",
    "for model in models:\n",
    "        #add this when doing this cell by itself\n",
    "        \n",
    "        train_residue_trans[model]=joblib.load(dir_in_data_mod+'train_residue_trans.pkl')\n",
    "        coeffs_temp = joblib.load(dir_in_data_mod+'AR(1)_coeffs.pkl')\n",
    "        \n",
    "        coeff_0[model] = coeffs_temp[0,:,:]\n",
    "        coeff_1[model] = coeffs_temp[1,:,:]\n",
    "        power_trans[model]=joblib.load(dir_in_data_mod+'yeo_johnson_pt_fmin_log.pkl')\n",
    "        \n",
    "        AR_process=np.zeros([train_residue_trans[model].reshape(-1,idx_l.sum()).shape[0]+120,\n",
    "                         idx_l.sum()]).reshape(-1,12,idx_l.sum())\n",
    "    \n",
    "        for t in np.arange(1,AR_process.shape[0]):\n",
    "            for i_mon in range(12):\n",
    "\n",
    "                if i_mon==0:\n",
    "                    AR_process[t,i_mon,:]=coeff_0[model][i_mon,:]+coeff_1[model][i_mon,:]*AR_process[t-1,11,:]\n",
    "                else:\n",
    "                     AR_process[t,i_mon,:]=coeff_0[model][i_mon,:]+coeff_1[model][i_mon,:]*AR_process[t,i_mon-1,:]\n",
    "\n",
    "        AR_process= AR_process[10:,:,:]\n",
    "\n",
    "        \n",
    "        train_residue_all_spat[model]=train_residue_trans[model].reshape(-1,12,idx_l.sum())-AR_process\n",
    "    \n",
    "        \n",
    "\n",
    "        # hardcoded for very slow leav-1-out cross val at moment to ensure to get most out of the data\n",
    "        nr_ts=nr_years\n",
    "        nr_folds = nr_ts*1\n",
    "        print('number folds', nr_folds)\n",
    "        fold_out_list = np.arange(nr_folds)\n",
    "        idx_fo_tot={}\n",
    "        j=0\n",
    "        for i in fold_out_list:      \n",
    "            idx_fo = np.zeros(nr_folds,dtype=bool)\n",
    "            idx_fo[j:j+1]=True\n",
    "            idx_fo_tot[i]=idx_fo    \n",
    "            j+=1    \n",
    "\n",
    "        # carry out cross-validation to determine the localisation radius L\n",
    "        print('start with localisation radius for',model)\n",
    "\n",
    "        df_llh_cv_all[model]={}\n",
    "        df_llh_cv_all[model]=Parallel(n_jobs=12,verbose=10)(delayed(leave_one_out)(L_set,nr_folds,train_residue_all_spat[model][:,i_mon,:],idx_fo_tot,phi) for i_mon in range(12))\n",
    "        \n",
    "        dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "\n",
    "        joblib.dump(df_llh_cv_all[model],dir_out_data_mod+'llh_cv_all.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
