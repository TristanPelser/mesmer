{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f6b64f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "r_home is None. Try python -m rpy2.situation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/tristan/mesmer/tools/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_single_mod\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AR1_predict, compute_llh_cv,gaspari_cohn\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaylorDiagram\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m## plotting\u001b[39;00m\n",
      "File \u001b[0;32m~/mesmer/tools/processing.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multivariate_normal\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# statistics which aren't all that nice in python\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobjects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrobjects\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAR_predict\u001b[39m(ts,alphas,p):\n\u001b[1;32m     27\u001b[0m \t\u001b[38;5;124;03m\"\"\" Make AR predictions for a given (set of) time series, alphas (AR coeffs) and AR order p.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\t\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\t\tKeyword arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m    \n",
      "File \u001b[0;32m~/miniconda3/envs/plotting/lib/python3.10/site-packages/rpy2/robjects/__init__.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01marray\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrinterface\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrinterface\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlike\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrlc\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobject\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RObjectMixin, RObject\n",
      "File \u001b[0;32m~/miniconda3/envs/plotting/lib/python3.10/site-packages/rpy2/rinterface.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrinterface_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openrlib\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrinterface_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rinterface_capi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_rinterface\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrinterface_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedded\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01membedded\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/plotting/lib/python3.10/site-packages/rpy2/rinterface_lib/openrlib.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     rlib \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mdlopen(lib_path)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rlib\n\u001b[0;32m---> 23\u001b[0m rlib \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen_rlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR_HOME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# R macros and functions\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_symbol_or_fallback\u001b[39m(symbol: \u001b[38;5;28mstr\u001b[39m, fallback):\n",
      "File \u001b[0;32m~/miniconda3/envs/plotting/lib/python3.10/site-packages/rpy2/rinterface_lib/openrlib.py:16\u001b[0m, in \u001b[0;36m_dlopen_rlib\u001b[0;34m(r_home)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"Open R's shared C library.\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r_home \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr_home is None. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTry python -m rpy2.situation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m rpy2\u001b[38;5;241m.\u001b[39msituation\u001b[38;5;241m.\u001b[39mget_rlib_path(r_home, platform\u001b[38;5;241m.\u001b[39msystem())\n\u001b[1;32m     19\u001b[0m rlib \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mdlopen(lib_path)\n",
      "\u001b[0;31mValueError\u001b[0m: r_home is None. Try python -m rpy2.situation"
     ]
    }
   ],
   "source": [
    "## general\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "import cf_units\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datetime as dt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "## statistics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal # to compute likelihood\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import shapiro  #check normalicy of seasonal trend distribution\n",
    "from scipy.optimize import curve_fit, fmin, fminbound, minimize, rosen_der, least_squares\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "##import functions for fitting\n",
    "from symfit import parameters, variables, Fit\n",
    "from symfit import pi,sqrt,log,exp,sinh\n",
    "from symfit import sin, cos\n",
    "\n",
    "\n",
    "# statistics which aren't all that nice in python\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "## my stuff\n",
    "sys.path.insert(1,'/home/tristan/mesmer/tools/')\n",
    "from loading import load_data_single_mod\n",
    "from processing import AR1_predict, compute_llh_cv,gaspari_cohn\n",
    "from plotting import TaylorDiagram\n",
    "\n",
    "\n",
    "## plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy.ma as ma\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import mplotutils as mpu\n",
    "\n",
    "##for parallelisation\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077df88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_fmin(self, X, y):\n",
    "    \"\"\"Estimate the optimal parameter lambda for each feature.\n",
    "    The optimal lambda parameter for minimizing skewness is estimated on\n",
    "    each feature independently using maximum likelihood.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data used to estimate the optimal transformation parameters.\n",
    "    y : temp values to calculate lmbda as lmbda = a*y + b\n",
    "    Returns\n",
    "    -------\n",
    "    self : object\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.copy()  # force copy so that fit does not change X inplace\n",
    "\n",
    "\n",
    "    self.coeffs_ =[]\n",
    "    for i_grid in tqdm(np.arange(idx_l.sum())):\n",
    "#         print(X.shape, y.shape)\n",
    "        self.coeffs_.append(yeo_johnson_optimize_fmin(self,X[:,i_grid],y[:,i_grid]))\n",
    "        \n",
    "        \n",
    "    self.coeffs_=np.array(self.coeffs_)\n",
    "    #print(self.coeffs_.shape)\n",
    "    self.mins_ = np.amin(X, axis=0)\n",
    "    self.maxs_ = np.amax(X, axis=0)\n",
    "    #print(self.coeffs_)\n",
    "    \n",
    "    if self.standardize:\n",
    "        self._scaler = StandardScaler(copy=True)\n",
    "        self._scaler.fit(X)\n",
    "        \n",
    "    return self\n",
    "    \n",
    "def transform_fmin(self, X, y):\n",
    "        \"\"\"Apply the power transform to each feature using the fitted lambdas.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The data to be transformed using a power transformation.\n",
    "        Returns\n",
    "        -------\n",
    "        X_trans : array-like, shape (n_samples, n_features)\n",
    "            The transformed data.\n",
    "        \"\"\"\n",
    "\n",
    "        lambdas=get_yeo_johnson_lambdas(self.coeffs_,y)\n",
    "        \n",
    "        X_trans=np.zeros_like(X)\n",
    "        for i, lmbda in enumerate(lambdas.T):\n",
    "            for j,j_lmbda in enumerate(lmbda):\n",
    "                with np.errstate(invalid='ignore'):  # hide NaN warnings\n",
    "                    X_trans[j, i] = self._yeo_johnson_transform(X[j, i], j_lmbda)\n",
    "\n",
    "        if self.standardize:\n",
    "            X_trans = self._scaler.transform(X_trans)\n",
    "\n",
    "        return X_trans\n",
    "\n",
    "def inverse_transform_fmin(self, X, y):\n",
    "        \"\"\"Apply the inverse power transformation using the fitted lambdas.\n",
    "        The inverse of the Yeo-Johnson transformation is given by::\n",
    "            if X >= 0 and lambda_ == 0:\n",
    "                X = exp(X_trans) - 1\n",
    "            elif X >= 0 and lambda_ != 0:\n",
    "                X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "            elif X < 0 and lambda_ != 2:\n",
    "                X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
    "            elif X < 0 and lambda_ == 2:\n",
    "                X = 1 - exp(-X_trans)\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The transformed data.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The original data\n",
    "        \"\"\"\n",
    "\n",
    "        if self.standardize:\n",
    "            X = self._scaler.inverse_transform(X) \n",
    "        \n",
    "        X_inv = np.zeros_like(X)\n",
    "        \n",
    "        lambdas=get_yeo_johnson_lambdas(self.coeffs_,y)\n",
    "        for i, lmbda in enumerate(lambdas.T):\n",
    "            for j,j_lmbda in enumerate(lmbda):\n",
    "                with np.errstate(invalid='ignore'):  # hide NaN warnings\n",
    "                    X_inv[j, i] = self._yeo_johnson_inverse_transform(X[j, i], j_lmbda)\n",
    "            X_inv[:,i]=np.where(X_inv[:,i]<self.mins_[i],self.mins_[i],X_inv[:,i])\n",
    "            X_inv[:,i]=np.where(X_inv[:,i]>self.maxs_[i],self.maxs_[i],X_inv[:,i])\n",
    "\n",
    "        return X_inv\n",
    "\n",
    "def yeo_johnson_optimize_fmin(self, x, y):\n",
    "    \"\"\"Find and return optimal lambda parameter of the Yeo-Johnson\n",
    "    transform by MLE, for observed data x.\n",
    "    Like for Box-Cox, MLE is done via the brent optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def _neg_log_likelihood(coeff):\n",
    "        \"\"\"Return the negative log likelihood of the observed data x as a\n",
    "        function of lambda.\"\"\"\n",
    "        lambdas=2/(1+coeff[0]*np.exp(y*coeff[1]))\n",
    "        \n",
    "        x_trans =np.zeros_like(x)\n",
    "#         print(x.shape)\n",
    "        #print(lambdas.shape)\n",
    "        for i, lmbda in enumerate(lambdas):\n",
    "            x_trans[i] = self._yeo_johnson_transform(x[i], lmbda)\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "\n",
    "        loglike = -n_samples / 2 * np.log(x_trans.var())\n",
    "        loglike += ((lambdas - 1) * np.sign(x) * np.log1p(np.abs(x))).sum()\n",
    "\n",
    "        return -loglike\n",
    "\n",
    "    # the computation of lambda is influenced by NaNs so we need to\n",
    "    # get rid of them\n",
    "    x = x[~np.isnan(x)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    # choosing bracket -2, 2 like for boxcox\n",
    "    bounds=np.c_[[0,0], [1,0.1]]\n",
    "    \n",
    "    return minimize(_neg_log_likelihood, np.array([0.01,0.001]), bounds=bounds, method='SLSQP',jac=rosen_der \n",
    "               ).x\n",
    "\n",
    "def get_yeo_johnson_lambdas(coeffs,y):\n",
    "\n",
    "    lambdas=np.zeros_like(y)\n",
    "    i=0\n",
    "    for a,b in zip(coeffs,y.T):\n",
    "        \n",
    "        lambdas[:,i]=2/(1+a[0]*np.exp(b*a[1]))\n",
    "        i+=1\n",
    "        \n",
    "    lambdas=np.where(lambdas<0,0,lambdas)\n",
    "    lambdas=np.where(lambdas>2,2,lambdas)\n",
    "    \n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100e32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def power_fit(residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        power_trans = fit_fmin(PowerTransformer(method='yeo-johnson'),residue.reshape(-1, idx_l.sum()),y)\n",
    "    else:\n",
    "        power_trans = PowerTransformer(method='yeo-johnson').fit(residue.reshape(-1, idx_l.sum()))\n",
    "    \n",
    "    return power_trans\n",
    "    \n",
    "def power_transform(mod, residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        residue_trans = transform_fmin(mod,residue.reshape(-1, idx_l.sum()),y).reshape(-1,idx_l.sum())\n",
    "    else:\n",
    "        residue_trans = mod.transform(residue.reshape(-1, idx_l.sum())).reshape(-1,idx_l.sum())\n",
    "            \n",
    "    return residue_trans  \n",
    "\n",
    "def power_inv_transform(mod, residue,y,fmin=True):\n",
    "    \n",
    "    if fmin:\n",
    "        residue_inv_trans = inverse_transform_fmin(mod,residue.reshape(-1, idx_l.sum()),y).reshape(-1,idx_l.sum())\n",
    "    else:\n",
    "        residue_inv_trans = mod.inverse_transform(residue.reshape(-1, idx_l.sum())).reshape(-1,idx_l.sum())\n",
    "            \n",
    "    return residue_inv_trans  \n",
    "\n",
    "def compute_llh_cv(res_tr,res_cv,phi):\n",
    "    \"\"\" Compute sum of log likelihood of a set of residuals based on a covariance matrix derived from a different set (of timeslots) of residuals\n",
    "    \n",
    "    Keyword arguments:\n",
    "        - res_tr: the residual of the training run lacking a specific fold after removing the local mean response (nr ts x nr gp). Nans must be removed before\n",
    "        - res_cv: the residual of a fold which was removed from the training run\n",
    "        - phi: matrix to localize the covariance matrix based on a specific localisation radius and distance information (phi = output of fct gaspari_cohen(geo_dist/L))\n",
    "    \n",
    "    Output:\n",
    "        - llh_innov_cv: sum of the log likelihood over the cross validation time slots\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ecov_res_tr = np.cov(res_tr,rowvar=False)\n",
    "    cov_res_tr=phi*ecov_res_tr\n",
    "    \n",
    "    mean_0 = np.zeros(phi.shape[0]) # we want the mean of the res to be 0\n",
    "\n",
    "    llh_innov_cv=np.sum(multivariate_normal.logpdf(res_cv,mean=mean_0, cov=cov_res_tr,allow_singular=True))\n",
    "\n",
    "    return llh_innov_cv   \n",
    "\n",
    "def leave_one_out(L_set,nr_folds,residue_trans,idx_fo_tot,phi):\n",
    "    \n",
    "    def folds_calc(idx_fo,residue_trans,phi,L):\n",
    "    \n",
    "        res_tot_est = residue_trans[~idx_fo] \n",
    "        res_tot_fo=residue_trans[idx_fo]\n",
    "\n",
    "        llh_cv=compute_llh_cv(res_tot_est,res_tot_fo,phi[L])\n",
    "        \n",
    "        return llh_cv\n",
    "    \n",
    "    idx_L=0\n",
    "    L = L_set[idx_L]\n",
    "    \n",
    "    df_llh_cv={}\n",
    "    df_llh_cv['llh_max']=-10000\n",
    "    df_llh_cv['all']={}\n",
    "    df_llh_cv['sum']={}\n",
    "    df_llh_cv['L_sel']=L_set[idx_L]\n",
    "    \n",
    "    while (L-df_llh_cv['L_sel']<=250) and (df_llh_cv['L_sel']<L_set[-1]): # based on experience I know that once stop selecting larger \n",
    "            #loc radii, will not start again -> no point in looping through everything, better to stop once max is \n",
    "            #reached (to avoid singular matrices)\n",
    "        L = L_set[idx_L]\n",
    "        print('start with L ',L)\n",
    "        df_llh_cv['all'][L]={}\n",
    "        df_llh_cv['sum'][L]=0\n",
    "        for i_fold_par in tqdm(np.arange(len(idx_fo_tot.keys()))):\n",
    "            df_llh_cv['all'][L][i_fold_par]=folds_calc(idx_fo_tot[i_fold_par],residue_trans,phi,L)\n",
    "            df_llh_cv['sum'][L] += df_llh_cv['all'][L][i_fold_par]\n",
    "            \n",
    "       \n",
    "        #df_llh_cv['all'][L]=Parallel(n_jobs=10,verbose=10)(delayed(folds_calc)(idx_fo_tot[i],residue_trans,phi,L)for i in np.arange(len(idx_fo_tot.keys())))\n",
    "        \n",
    "        #print('rest tot fo shape ',res_tot_fo.shape,'res_tot_est shape ',res_tot_est.shape)\n",
    "        if df_llh_cv['sum'][L]>df_llh_cv['llh_max']:\n",
    "            df_llh_cv['L_sel']=L\n",
    "            df_llh_cv['llh_max']=df_llh_cv['sum'][L]\n",
    "            print('currently selected L=',df_llh_cv['L_sel'])\n",
    "\n",
    "        idx_L+=1  \n",
    "    return df_llh_cv\n",
    "\n",
    "def lin_func(x, a, b):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21914f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the land mask as frac_l\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dir_in_geo_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/tristan/mesmer/data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m frac_l \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241m.\u001b[39mopen_mfdataset(dir_in_geo_dist \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterim_invariant_lsmask_regrid.nc\u001b[39m\u001b[38;5;124m'\u001b[39m, combine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby_coords\u001b[39m\u001b[38;5;124m'\u001b[39m,decode_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m frac_l_raw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(copy\u001b[38;5;241m.\u001b[39mdeepcopy(frac_l\u001b[38;5;241m.\u001b[39mlsm\u001b[38;5;241m.\u001b[39mvalues))  \u001b[38;5;66;03m#land-sea mask of ERA-interim bilinearily interpolated \u001b[39;00m\n\u001b[1;32m      7\u001b[0m frac_l \u001b[38;5;241m=\u001b[39m frac_l\u001b[38;5;241m.\u001b[39mwhere(frac_l\u001b[38;5;241m.\u001b[39mlat\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# remove Antarctica from frac_l field (ie set frac l to 0)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "# load the land mask as frac_l\n",
    "dir_in_geo_dist = '/home/tristan/mesmer/data/'\n",
    "frac_l = xr.open_mfdataset(dir_in_geo_dist + 'interim_invariant_lsmask_regrid.nc', combine='by_coords',decode_times=False)\n",
    "\n",
    "frac_l_raw = np.squeeze(copy.deepcopy(frac_l.lsm.values))  #land-sea mask of ERA-interim bilinearily interpolated \n",
    "\n",
    "frac_l = frac_l.where(frac_l.lat>-60,0)  # remove Antarctica from frac_l field (ie set frac l to 0)\n",
    "\n",
    "idx_l=np.squeeze(frac_l.lsm.values)>0.0 # idx_l = index land -> idex land #-> everything >0 we consider as land\n",
    "\n",
    "lon_pc, lat_pc = mpu.infer_interval_breaks(frac_l.lon, frac_l.lat)  ## is this needed??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd50380",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m BEST_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs_data_25.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m data_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterim_invariant_lsmask_regrid.nc\u001b[39m\u001b[38;5;124m'\u001b[39m       \n\u001b[0;32m---> 10\u001b[0m df_obs \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241m.\u001b[39mopen_mfdataset(dir_in_geo_dist\u001b[38;5;241m+\u001b[39mBEST_data)\u001b[38;5;241m.\u001b[39mroll(lon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m72\u001b[39m) \u001b[38;5;66;03m#open observation data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#create the climatology values array\u001b[39;00m\n\u001b[1;32m     13\u001b[0m y_ma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((tot_months,idx_l\u001b[38;5;241m.\u001b[39msum()))  \u001b[38;5;66;03m#create emtpy array with correct shape\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "dir_in_data_mod = '/home/tristan/mesmer/data/'\n",
    "nr_yrs = 112\n",
    "nr_months = 12\n",
    "tot_months = nr_yrs*nr_months\n",
    "\n",
    "# prepare the inputs as array\n",
    "BEST_data = 'obs_data_25.nc'\n",
    "data_mask = 'interim_invariant_lsmask_regrid.nc'       \n",
    "\n",
    "df_obs = xr.open_mfdataset(dir_in_geo_dist+BEST_data).roll(lon=72) #open observation data\n",
    "\n",
    "#create the climatology values array\n",
    "y_ma = np.zeros((tot_months,idx_l.sum()))  #create emtpy array with correct shape\n",
    "for i in range(tot_months):\n",
    "    y_ma[i] = df_obs.climatology.values[i%12,idx_l]    #fill climatology values in the array\n",
    "\n",
    "#create test data over date range - here, 127 years so 1910 incl. to 2022 incl. \n",
    "## 1344 is the number of months from 1910 until 2022\n",
    "data_test = np.nan_to_num(np.array([df_obs.temperature.values[720:2064,idx_l]]))\n",
    "data_test = data_test.reshape(tot_months,idx_l.sum())\n",
    "\n",
    "print(data_test.shape)\n",
    "\n",
    "#load in monthly temperature values by adding the temp anomolies to the climatology\n",
    "y_all_mon = np.add(y_ma, data_test)     \n",
    "\n",
    "# now subtract the yearly average climatology so we are left with residuals\n",
    "y_all_mon = y_all_mon - np.reshape(np.tile(np.mean(df_obs.climatology.values[:,idx_l],axis=0),tot_months),(tot_months,idx_l.sum()))   \n",
    "\n",
    "################## UNTIL HERE #########################             \n",
    "\n",
    "#calculate annual average temperature values- here we use nanmean to calculate the annual means for each gridpoint but skipping any Nan values\n",
    "y_all = np.mean(y_all_mon.reshape(-1,12,idx_l.sum()),axis=1)\n",
    "print(y_all.shape)\n",
    "\n",
    "joblib.dump(y_all, dir_in_data_mod+'y_all.pkl')\n",
    "joblib.dump(y_all_mon, dir_in_data_mod+'y_all_mon.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_res={}\n",
    "nr_emus=500\n",
    "buffer=10\n",
    "nr_ts=112\n",
    "dir_in_data_mod = '/home/tristan/mesmer/output/'\n",
    "dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "\n",
    "##load calibration parameters for local variability module\n",
    "df_llh_cv_all= joblib.load(dir_in_data_mod+'llh_cv_all.pkl')\n",
    "coeffs_temp = joblib.load(dir_in_data_mod+'AR(1)_coeffs.pkl')\n",
    "coeff_0 = coeffs_temp[0,:,:]\n",
    "coeff_1 = coeffs_temp[1,:,:]\n",
    "power_trans=joblib.load(dir_in_data_mod+'yeo_johnson_pt_fmin_log.pkl')\n",
    "train_residue_trans=joblib.load(dir_in_data_mod+'train_residue_trans.pkl')\n",
    "innov_emu = joblib.load(dir_in_data_mod+'innov_emu.pkl')\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "for k in np.arange(nr_emus):\n",
    "    emu_res[k]=np.zeros([nr_ts+buffer,12,idx_l.sum()])\n",
    "    for t in np.arange(1,emu_res[k].shape[0]):\n",
    "        for i_mon in range(12):\n",
    "\n",
    "            if i_mon==0:\n",
    "                emu_res[k][t,i_mon,:]=coeff_0[i_mon,:]+coeff_1[i_mon,:]*emu_res[k][t-1,11,:]+innov_emu[i_mon][k,t]\n",
    "\n",
    "            else:\n",
    "                emu_res[k][t,i_mon,:]=coeff_0[i_mon,:]+coeff_1[i_mon,:]*emu_res[k][t,i_mon-1,:]+innov_emu[i_mon][k,t]\n",
    "\n",
    "    emu_res[k]=emu_res[k][buffer:,:,:]\n",
    "\n",
    "    for i_mon in range(12):\n",
    "\n",
    "        emu_res[k][:,i_mon,:]=power_inv_transform(power_trans[i_mon],emu_res[k][:,i_mon,:],y_all)\n",
    "\n",
    "joblib.dump(emu_res,dir_out_data_mod+'%i_emulator_innovations_fmin_log.pkl'%(nr_emus))\n",
    "time_taken = dt.datetime.now() - start\n",
    "\n",
    "print('time taken to create %i emulations: '%(nr_emus, time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a076e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emulations(y_all, innov_emu, power_trans):\n",
    "    emu_res=np.zeros([nr_ts+buffer,12,idx_l.sum()])\n",
    "    for t in np.arange(1,emu_res.shape[0]):\n",
    "        for i in range(12):\n",
    "            if i_mon==0:\n",
    "                    emu_res[t,i_mon,:]=coeff_0[i_mon,:]+coeff_1[i_mon,:]*emu_res[t-1,11,:]+innov_emu[i_mon][t]\n",
    "\n",
    "                else:\n",
    "                    emu_res[t,i_mon,:]=coeff_0[i_mon,:]+coeff_1[i_mon,:]*emu_res[t,i_mon-1,:]+innov_emu[i_mon][t]\n",
    "\n",
    "        emu_res=emu_res[buffer:,:,:]\n",
    "\n",
    "        for i_mon in range(12):\n",
    "\n",
    "            emu_res[:,i_mon,:]=power_inv_transform(power_trans[i_mon],emu_res[:,i_mon,:],y_all)\n",
    "    \n",
    "    return emu_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5570de",
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_res={}\n",
    "nr_emus=500\n",
    "buffer=10\n",
    "nr_ts=112\n",
    "dir_in_data_mod = '/home/tristan/mesmer/output/'\n",
    "dir_out_data_mod = '/home/tristan/mesmer/output/'\n",
    "\n",
    "##load calibration parameters for local variability module\n",
    "df_llh_cv_all= joblib.load(dir_in_data_mod+'llh_cv_all.pkl')\n",
    "coeffs_temp = joblib.load(dir_in_data_mod+'AR(1)_coeffs.pkl')\n",
    "coeff_0 = coeffs_temp[0,:,:]\n",
    "coeff_1 = coeffs_temp[1,:,:]\n",
    "power_trans=joblib.load(dir_in_data_mod+'yeo_johnson_pt_fmin_log.pkl')\n",
    "train_residue_trans=joblib.load(dir_in_data_mod+'train_residue_trans.pkl')\n",
    "innov_emu = joblib.load(dir_in_data_mod+'innov_emu.pkl')\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "for k in np.arange(nr_emus):\n",
    "    emu_res[k] = create_emulations(y_all, innov_emu, power_trans)\n",
    "    \n",
    "print('time taken to create %i emulations: '%(nr_emus, time_taken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
